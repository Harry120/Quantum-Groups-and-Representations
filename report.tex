\documentclass[a4paper]{article}

\title{MATH3001}
\author{Harry Partridge}
\date{Term 1 2021}
\def\descrip{Quantum Mechanics and Representation Theory}

\input{header}

\begin{document}

\maketitle

\tableofcontents

\newpage
\section{Introduction}
The purpose of this report is to explore representation theory and examine implications for quantum mechanics. The key result here is that quantum observables arise from representations of Lie groups on a Hilbert space. This process occurs in the following way:
\begin{enumerate}
    \item Suppose $G$ is a Lie group. The tangent space at the identity $e \in G$ is a vector space that can be made into a lie algebra $\g$. 
    \item Exponentiating elements $A$ of the lie algebra gives lie group elements $g = e^A$ in the connected component of the identity. 
    \item Suppose $G$ acts unitarily on a Hilbert space $\H$. Then $e^{(A^\dagger)} = \left(e^{A}\right)^\dagger = e^{-A}$, so $A^\dagger = -A$ and $A$ is skew adjoint. 
    \item A new self adjoint operator $O = iA$ can be defined, and this will be an observable of the quantum system. 
\end{enumerate}
Some examples that we will explore 
\begin{enumerate}
    \item Say $\H$ is $L^2(\R^3)$. Then $SO(3)$ acts on $\H$ by $(\pi(R)f)(x) = f(R^{-1}x)$. This generates the orbital angular momentum operators.
    \item Say $\H$ is $\C^n$. Then $SU(2)$ acts on $\H$ by $(\pi(R)f)(x) = f(R^{-1}x)$. This generates the spin angular momentum operators.
    \item Say $\H$ is $\C$. Then $U(1)$ acts on $\H$ by $\rho(e^{i\theta} = e^{ik\theta})$, $k$ an integer.
    \item $G = \R$ - time?
\end{enumerate}

\section{Representations of finite groups}

\subsection{Basic Definitions}
There are several equivalent ways to define a representation of a group \cite{dexter}. The first is via a homomorphism. 
\begin{defi}[Representation]
    For a given group $G$, a \textbf{representation} is a group homomorphism $$\rho: G \to GL(V),$$ where $V$ is a vector space over a field $\F$. 
\end{defi}

Alternatively, a representation can also be viewed as a linear action on a vector space.
\begin{defi}[Linear action]
    A group $G$ \textbf{acts linearly} on a vector space $(V, \F)$ if acts on the set $V$ such that $$g(\vec{v}+ \vec{u}) = g\vec{v} + g\vec{u} \text{ and } g(\lambda \vec{v}) = \lambda g\vec{v}$$
    for all $\vec{u}, \vec{v} \in V$ and $\lambda \in \F$. Such an action gives a homomorphism to $GL(V)$ via $\rho(g)(\vec{v}) = g\vec{v}$, and any homomorphism to $GL(V)$ also gives a linear action in an analogous way.
\end{defi}

Lastly, a representation can also be viewed as a module over a the group algebra $\F G$.
\begin{defi}[Group algebra]
    For a given group $G$, the group algebra $\F G$ is 
    $$\F G = \set{\sum_{g \in G} \alpha_g g: \alpha_g \in \F}.$$ This is another way of describing a linear action, and gives a homomorphism to $GL(V)$ in the same way.
\end{defi}

Regardless of how a representation is defined, it has a \textbf{dimension} $n$ which is equal to dim$_{\F}(V)$. As a choice of basis gives a natural isomorphism from $V$ to $\F^n$, any representation also has an associated \textbf{matrix representation}. 
\begin{defi}[matrix representation]
    A matrix representation is a homomorphism $R$ from $G$ to $GL_n(\F)$. If $V$ is an $n$ dimensional vector space with $\theta: V \to \F^n$ an isomorphism, then $$\rho(g)(\vec{v}) =  \theta^{-1}(R(g)\theta(\vec{v})))$$ is an associated representation on $V$. 
\end{defi}

If $V$ is a vector space with a representation from $G$ to $GL(V)$, then we say that $V$ is a $G$-space. Furthermore, if only the identity acts trivially on $V$ (equivalently if the kernel of the homomorphism to $GL(V)$ is $\{e\}$), then we say that the representation is \textbf{faithful}.

We now prove some results about the eigenvalues and eigenvectors of representations.

\begin{lemma}
    Suppose that $\rho : G \to GL(V)$ is a representation of a finite group $G$. Then for every $g \in G$, any eigenvalue $\lambda$ of $\rho(g)$ satisfies $\lambda^n = 1$.
\end{lemma} 

\begin{proof}
    Suppose that under the associated linear action of $G$ on $V$, $\lambda$ is an eigenvalue of $g$ with eigenvector $\vec{v}$. Then $g^n\vec{v} = \lambda^n \vec{v}$. As $G$ is a finite group, $g^n = e$ for some $n$, so $\lambda^n = 1$.
\end{proof}
In particular, $\F = \R$, then all representations have eigenvalues that are either $1$ or $-1$, and if $\F = \C$, then eigenvalues are roots of unity.

\begin{cor}
    Suppose that $\rho : G \to GL(V)$ is a complex representation of a finite group $G$. Then for every $g \in G$, $\rho(g)$ is diagonalizable. \textit{does this work in all algebraically closed fields?} 
\end{cor} 

\begin{proof}
    According the characteristic polynomial of $\rho(g)$ must divide $t^n - 1$. As this polynomial has no repeated roots, $\rho(g)$ must be diagonalizable.
\end{proof}

\subsection{Subrepresentations}
As with other algebraic topics, there is a lot that can be learned about representations by decomposing them into `simpler' parts.

\begin{defi}[G-subspace]
    For a given representation $\rho: G \to GL(V)$ of $G$, we say that $W \leq V$ is a $G$-subspace if it is a $\rho(G)$ invariant subspace of $V$: $$\rho(g)(W) \leq W$$ for all $g \in G$.
\end{defi}

Clearly, any $G$-subspace gives us another representation.

\begin{defi}
    If $W \leq V$ is a $G$-subspace for the representation $\rho$, then the map $\rho_{|W}: G \to GL(V)$ given by $\rho_{|W}(g) = \rho(g)|_{W}$ is a \textbf{subrepresentation} of $\rho$.
\end{defi}

A $G$-subspace is \textbf{proper} if it is not all of $V$, and \textbf{trivial} if it is just the zero vector.

\begin{defi}[Reducible]
    A representation $\rho: G \to GL(V)$ is said to be \textbf{reducible} if there are proper non-trivial \textit{G-subspaces}.
\end{defi}

A stronger requirement is for $V$ to be the sum of such \textit{G-subspaces}.

\begin{defi}[Decomposable]
    A representation $\rho$ is said to be \textbf{decomposable} if there are proper non-trivial \textit{G-subspaces} $U,W \leq V$ such that $$V = U \oplus W.$$ We then say that $\rho = \rho_W \oplus \rho_W$.
\end{defi}

A priori, it is not clear that reducibility implies decomposability. This is however, content of Maschke's Theorem, which we will prove later.

\subsection{Equivalent Representations}
We would like to describe a level of equivalence between two G-spaces. As usual, this is done by considering isomorphisms.

\begin{defi}[G-homomorphism]
    Suppose $V$ and $W$ are two G-spaces with representations $\rho_V$ and $\rho_W$. If $f: V \to W$ is a linear map such that $$f(\rho_V(g)\vec{v}) = \rho_W(g)f(\vec{v}),$$ we say that $f$ is a \textbf{G-homomorphism}, and that \textbf{intertwines} $\rho_V$ and $\rho_W$. If $f$ is also an isomorphism of vector spaces, then we say that $V$ and $W$ are \textbf{equivalent} and that $f$ is a \textbf{G-isomorphism}.
\end{defi}

Evidently, if $R_1$ and $R_2$ are two matrix representations of $G$ associated to the same representation $\rho$ on $V$ by isomorphisms $\theta_1$ and $\theta_2$, then they are equivalent, because $\theta_2 \theta_1^{-1}$ is a $G$-isomorphism that intertwines $R_1$ and $R_2$. 

\subsection{Examples of Representations}
We now list some common representations. 

The \textbf{trivial representation} $1_G$ is defined by $1_G(g) = (id : \F \to \F)$ for any field $\F$. Note that the trivial representation is always one dimensional. One may also speak of `a trivial representation on $V$' which would be taken to mean $\rho(g) = $id$_V$.

A more complicated type of representation is a \textbf{permutation representation}. For any set $X$, we can define a vector space $\F X$ as a set of formal sums $$\F X = \set{\sum_{x \in X} \alpha_x x : \alpha_x \in \F},$$ with addition and scalar multiplication defined in the obvious way. 
If a group $G$ acts on a set $X$, we can therefore define a linear action on $\F X$ by $$g\sum_{x \in X} \alpha_x x = \sum_{x \in X} \alpha_x gx.$$ The corresponding representation is called a permutation representation of $G$.

When $X = G$ with the action of $G$ corresponding to left multiplication, we get a particularly special type of permutation representation $\rho_{reg}$ called the \textbf{regular representation}. 

\begin{eg}[Representations of $S_3$] 
    $S_3$ acts naturally on the set $X = \{a,b,c\}$, which gives us a three dimensional permutation representation. The natural associated matrix representation are three by three permutation matrices. Contrastingly, the regular representation of $S_3$ is six dimensional, and its natural associated matrices are a subset of the six by six permutation matrices.
\end{eg}

\subsection{Complete reducibility for finite groups}

In this section, we will show that every finite dimensional representation of a finite group over a field of characteristic zero can be decomposed into a sum of irreducible representations.

\begin{thm}[Maschke's Theorem]
    Suppose $G$ is a finite group with a linear action on a finite dimensional vector space $V$ over $\F$ with (char$(\F), |G|) = 1$. Then if $W$ is a $G$-subspace of $V$, there exists a $G$-subspace $U$ of $V$ such that $V = W \oplus U$.
\end{thm}

\begin{proof} Suppose $f: V \twoheadrightarrow W$ is an endomorphism on $V$ defined by $f(w) = w$ for $w \in W$. Then because (char$(\F), |G|) = 1$, $\frac{1}{|G|}$ is well defined, so we can set  
$$\bar{f}(v) = \frac{1}{|G|}\sum_{g\in G} gf(g^{-1}v).$$
We claim that $V = W \oplus$ $\ker(\bar{f})$. First of all, since $f(g^{-1}v) \in W$, and $W$ is $G$ invariant, $\im(\bar{f}) \subseteq W$. Furthermore, note that for any $w \in W$, $g^{-1}(w) \in W$, so 
\begin{align*}
\bar{f}(w) &= \frac{1}{|G|}\sum_{g\in G} g f(g^{-1}w)\\ 
&= \frac{1}{|G|}\sum_{g\in G} g g^{-1}w\\ 
&= \frac{1}{|G|}\sum_{g\in G} w\\ 
&= w.
\end{align*}
Therefore $\im(\bar{f}) = W$, and $\bar{f}^2 = \bar{f}$. Then for any $v \in V$, $\bar{f}(\bar{f}(v) - v) = 0$, so $\bar{f}(v) - v \in \ker(\bar{f})$. Hence $v = w + u$ where $u \in \ker(\bar{f})$ and $w = \bar{f}(v) \in W$, so $V = W + \ker(\bar{f})$. Now suppose $y \in W \cap \ker(\bar{f})$. Then $0 = \bar{f}(y) = y$, so $V = W \oplus \ker(\bar{f})$.

Furthermore, if $v \in \ker(\bar{f})$, then for any $h \in G$, 
\begin{align*}
\bar{f}(hv)  &= \frac{1}{|G|}\sum_{g\in G} g f(g^{-1}hv)\\ 
    &= \frac{1}{|G|}\sum_{g'\in G} hg'f(g'^{-1}v)\\ 
    &= h\bar{f}(v)\\ 
    &= 0.
\end{align*}
Therefore $\ker(\bar{f})$ is $G$-invariant, and the theorem is proved. 
\end{proof} 

\begin{cor}
    Any finite dimensional complex representation of a finite group $G$ is \textbf{completely reducible} (\textbf{semisimple}): there exist irreducible $G$-subspaces $V_1, \hdots V_n$ such that $V \simeq V_1 \oplus \hdots \oplus V_n$.
\end{cor}

\begin{proof} If $V$ is one dimensional, it is irreducible so we are done. Otherwise, if $V$ is reducible, use Maschke's theorem to write $V = U \oplus W$, where $\dim(U) < \dim(V)$ and  $\dim(W) < \dim(V)$. The theorem then follows by induction on the dimension.
\end{proof}

\subsection{Schur's lemma} 

Schur's lemma essentially says that irreducible representations of a group cannot be intertwined in a non trivial way. Crucially, the second part of Schur's lemma requires that $\F$ is an algebraically closed field, so it does not apply over $\R$.

\begin{lemma}[Schur's lemma]
    Suppose $V$ and $W$ are $G$-invariant vector spaces. Then if $f:V \to W$ is a $G$-homomorphism, then $f$ is either the zero map or an isomorphism. In particular, if $W = V$ and $\F$ is an algebraically closed field, then $f$ is a scalar multiple of the identity map.
\end{lemma}

\begin{proof}
    Suppose $f$ is a $G$-homomorphism between $G$-spaces $V$ and $W$. Then $\ker(f)$ is a $G$-subspace of $V$, so irreducibility implies that either $\ker(f) = V$, in which case $f = 0$, or $\ker(f) = \{0\}$, in which case $f$ is injective. Similarly, if $f$ is not the zero map, then $\im(f)= W$, so $f$ is surjective and therefore an isomorphism. 

    If $V = W$, then the algebraic closure of $\F$ implies that $f$ has an eigenvalue $\lambda$. Therefore $f - \lambda I$ is a $G$-homomorphism with non zero kernel, and is therefore the zero map. Hence $f - \lambda I = 0$, so $f = \lambda I$.
\end{proof}

Schur's lemma has a number of important corollaries. For these results, assume that the vector space is over an algebraically closed field.

\begin{cor}
    If $G$ is abelian, then its irreducible representations must be one dimensional.
\end{cor}

\begin{proof}
    Suppose $\rho:G \to GL(V)$ is an irreducible representation of an abelian group $G$. Then for every $g \in G$, commutativity implies that $\rho(g)\rho(h) = \rho(h)\rho(g)$, so $\rho(h) \in \End(V)$ is a $G$-homomorphism, so $\rho(h)$ is a scalar multiple of the identity map, and every subspace of $V$ is $G$-invariant. Therefore $V$ must be one dimensional.
\end{proof}

\begin{cor}
    $Z(G)$ is precisely the set of $g \in G$ such that if $\rho$ is a faithful irreducible representation, then $\rho(g) = \lambda \id_V$, where $\lambda \in \F$. 
\end{cor}

\begin{proof}
    If $g \in Z(G)$, then $\rho(g)$ is a $G$-homomorphism, so according to Schur's lemma $\rho(g) = \lambda \id_V$ for some eigenvalue $\lambda \in \F$.

    Conversely, if $\rho(g) = \lambda \id_V$, then $\rho(gh) = \lambda \id_V \rho(h) = \rho(h) \lambda \id_V = \rho(hg)$. As $\rho$ is faithful and therefore also injective, this implies that $gh = hg$, so $g \in Z(G)$.
\end{proof}

\subsection{Character Theory}
An essential result from linear algebra says that for square matrices, trace is a similarity invariant: if $R_1 = MR_2M^{-1}$ for some matrix $M$, then $\tr(R_1) = \tr(R_2)$. Intuitively, this makes sense because the trace is the sum of eigenvalues, and 
since eigenvalues are defined without reference to a basis, the trace should be basis independent. This means that for a given representation $\rho : G \to GL(V)$, $\tr(\rho(g))$ is well defined for every $g \in G$. For every representation we therefore have an associated function $\chi : G \to GL(V) \to \F$ given by $\chi = \tr \circ \rho$. This is known as the character of $G$.

\begin{defi}[Character]
    Suppose $\rho : G \to GL(V)$ is a representation of a finite group $G$ on a vector space $V$ with field $\F$. Then we say that $\rho$ affords a \textbf{character} $\chi: G \to \F$ defined by $\chi(g) = \tr(\rho(g))$ for every $g \in G$.
\end{defi}

Characters are incredibly useful because they turn out to be a complete invariant: a representation is uniquely determined by its character. We begin with some basic definitions and results. 

\begin{defi}[Degree]
    The \textbf{degree} of a character $\chi : G \to GL(V) \to \F$ is the dimension of the vector space $V$.
\end{defi}

\begin{defi}
    A character $\chi$ is said to be \textbf{irreducible} iff its associated representation $\rho$ is irreducible. Similarly, $\chi$ is said to be \textbf{faithful} iff $\rho$ is. 
\end{defi}

An initial observation is that if $\chi$ has degree $n$, then for any $g \in \ker(\rho)$, we see that $\chi(g) = \tr(\id_V) = \dim(V) = n$. 

For complex representations, we can say a bit more. Thanks to a lemma in section 2.1, we know that any eigenvalue $\lambda$ of $\rho(g)$ must be a root of unity. This allows us to bound $|\chi(g)|$ above by applying the Cauchy-Schwartz inequality: $|\chi(g)| = \left| \sum_i \lambda_i \right| \leq \sum_i \left| \lambda_i \right| = \dim(V) = n$. As equality holds when all $\lambda_i$ are equal, this also says that $|\chi(g)| = n$ iff $\rho(g) = \lambda \id_V$ for an eigenvalue $\lambda$. 

We can now improve on the final corollary in section 2.6 and say that $Z(G)$ is precisely the set where $|\chi(g)| = n$ for any faithful and irreducible character $\chi$.

\begin{lemma}
    Suppose $\rho$ and $\rho'$ are two representations with characters $\chi$ and $\chi'$. Then if $\rho$ is equivalent to $\rho'$, then $\chi = \chi'$.
\end{lemma}

\begin{proof}
    By equivalence, $\rho(g) = f \circ \rho'(g) \circ f^{-1}$ for some isomorphism $f$. Hence $\chi(g) = \tr(f \circ \rho'(g) \circ f^{-1}) = \chi'(g)$. 
\end{proof}

\begin{defi}[Class functions]
    For a group $G$, the space of class functions over $\F$ is the set of functions from $G$ to $\F$ that are constant on the conjugacy classes $\class_j$ of $G$: $$\class(G) = \{f : G \to \F : f(ghg^{-1}) = f(h) \text{ for all } g, h \in G\}.$$
    This set is made into a vector space under the normal addition and scalar multiplication of functions. By considering the class functions as column vectors with entries in $\F$, we can see that $\class(G)$ is isomorphic to $\F^k$, where $k$ is the number of conjugacy classes of $G$.
\end{defi}

\begin{lemma}
    Any character $\chi$ is a class function.
\end{lemma}

\begin{proof}
    By definition, $\chi(ghg^{-1}) = \tr\left(\rho(g)\rho(h)\rho(g)^{-1}\right) = \chi(h)$. 
\end{proof}

When taken over the complex numbers, $\class(G)$ can be made into a hermitian inner product space (In fact, $\class(G)$ is then a Hilbert space, as all norms on $\C^k$ are equivalent) by defining 
$$\anglet{f}{f'} = \frac{1}{|G|}\sum_{g\in G} \overline{f(g)}f'(g).$$
\textit{How can this be extended consistently to all fields where $(\ch(\F), |G|) = 1$?}

The main result of character theory is that the irreducible characters of a group are orthonormal with respect to this inner product.

\begin{thm}[Completeness of Characters]
    Over the complex numbers, the irreducible characters of $G$ form an orthonormal basis for $\class(G)$: $\anglet{\chi_i}{\chi_j} = \delta_{ij}$ and every $f \in \class(G)$ can be written as $\sum_i \alpha_i \chi_i$ for some coefficients $\alpha_i$.
\end{thm}

\begin{proof}
    TO DO.
\end{proof}

\begin{cor}
    The number of conjugacy classes of $G$ is the number of complex irreducible characters.
\end{cor}

\begin{proof}
    The irreducible characters form a basis for $\class(G) \simeq \C^k$, so there must be exactly $k$ such characters.
\end{proof}

\begin{cor}
    Complex representations of finite groups are uniquely determined by their characters.
\end{cor}

\begin{proof}
    TO DO.
\end{proof}

\begin{cor}[Irreduciblity Criterion]
    A complex representation $\rho$ of a finite group $G$ affording character $\chi$ is irreducible iff $\anglet{\chi}{\chi} = 1$.
\end{cor}

\begin{proof}
    Suppose $\rho$ is irreducible. Then so is $\chi$ and $\anglet{\chi}{\chi} = 1$. 

    If $\anglet{\chi}{\chi} = 1$, then since $\chi = \sum m_i \chi_i$, we must have that $\sum m_i^2 = 1$ for some integers $m_0$. Then exactly one $m_i=1$, and the rest are zero, so $\chi$ is irreducible.
\end{proof}

\begin{thm}
    sum of squares of dimensions is size of G.
\end{thm}

\begin{proof}
    TO DO.
\end{proof}

To conclude this section, we look at an example.

\begin{eg}[Irreducible representations of $S_3$]
    TO DO.
\end{eg}

\section{Lie Theory}
\subsection{Representations of Topological Groups}
A topological group is a group that has been endowed with a topology. Most of the definitions regarding representations of topological groups are very similar to those for finite groups, but typically also require some kind of continuity. 
The following definitions are given by \cite{alex}.

\begin{defi}[Topological group]\end{defi}
\begin{enumerate}
    \item A group $G$ endowed with a Hausdorff topology is a \textbf{topological group} if 
    \begin{enumerate}
        \item The left and right multiplication maps $L_g : G \to G : h \mapsto gh$ and $R_g : G \to G : h \mapsto hg$ are continuous for every $g$.
        \item The inverse map $\inv: G \to G : g \mapsto g^{-1}$  is continuous.
    \end{enumerate}
    \item A \textbf{morphism} between topological groups is a continuous group homomorphism.
\end{enumerate}

Primarily, we are interested in the case where $G$ is compact under the topology $\tau$. Some examples of compact topological groups include all finite groups with discrete topology, discrete groups such as $\Z$ (also with discrete topology), and Lie groups. 

\begin{defi}[G-space]\end{defi}
\begin{enumerate}
    \item A locally compact topological space $X$ is a $G$\textbf{-space} if there is a continuous map $X \to X$ given by $x \mapsto gx$ for every $g\in G$ and
    \begin{enumerate}
        \item $g_1 (g_2 x) = (g_1 g_2)x$ for all $g_1, g_2 \in G$
        \item $ex = x$ where $e$ is the identity in $G$.
    \end{enumerate} 
    \item A \textbf{morphism} between two $G$-spaces $X$ and $Y$ is a continuous map $\phi: X \to Y$ satisfying $\phi(gx) = g \phi(x)$ for all $g \in G, x \in X$. 
\end{enumerate}

Representations of $G$ arise when $X$ is also endowed with a vector space structure. 

\begin{defi}[Topological vector space]
    A \textbf{topological vector space} is a vector space equipped with a Hausdorff topology for which addition and scalar multiplication are continuous.
\end{defi}

\begin{eg}
    Suppose that $V$ is a finite dimensional vector space. Then the standard topology inherited via a linear isomorphism $V \simeq R^n$ is the unique topology which makes $V$ into a topological vector space. \cite{alex}
\end{eg}

For our purposes, the most important example of a topological vector space is a Hilbert space.

\begin{defi}[Hilbert Space]
    Suppose $V$ is a vector space endowed with an inner product $\anglet{\cdot}{\cdot}$. Then $V$ can be made into a topological vector space by the the topology induced by the metric $d(x, y) = \sqrt{\anglet{x}{y}}$. $V$ is then a \textbf{Hilbert space} if it is complete with respect to the metric $d$. 
\end{defi}

For any topological vector space $V$, we can define $GL(V)$ to be the set of continuous automorphisms on $V$. This set is itself a finite dimensional vector space, and is therefore made into a topological vector space in the canonical way.

As is the case with finite groups, representations of topological groups can be viewed as either homomorphisms, linear actions or modules. We give one definition here; the other viewpoints relate in the same way as before.

\begin{defi}[G-representation]\end{defi}
\begin{enumerate}
    \item Suppose $G$ is a topological group. A \textbf{representation} of $G$ on a topological vector space $V$ is a continuous homomorphism $\pi: G \to GL(V)$. The induced linear action of $G$ on $V$ is given by $gv = \pi(g)v$.
    \item A \textbf{morphism} between $G$-representations $\pi_1$ and $\pi_2$ on $V_1$ and $V_2$ respectively is continuous linear map $T: V_1 \to V_2$ such that $T \circ \pi_1(g) = \pi_1(g) \circ T$ for all $g\in G$. It is clear that this is a $G$-space morphism when $V_1$ and $V_2$ are considered $G$-spaces under the actions induced by $\pi_1$ and $\pi_2$.
\end{enumerate}

If $V$ is a Hilbert space, we have a particularly important kind of representation.

\begin{defi}[Unitary representation]
    A representation $\pi$ of $G$ on a Hilbert space $\H$ is \textbf{unitary} if $\anglet{gv}{gw} = \anglet{v}{w}$ for all $v, w \in V$ and $g \in G$.
\end{defi}

\subsection{Lie groups}
We begin by reviewing some manifold theory.

\begin{defi}[Manifold]
    Let $M$ be a set with a second countable Hausdorff topology $\tau$. A $n$ dimensional smooth structure on $M$ consists of the following elements:
    \begin{enumerate}
        \item A collection of connected open sets $V_\alpha \in \tau$ whose union is $M$. 
        \item A collection of homomorphisms $\phi_\alpha : V_\alpha \to U_\alpha$, where $U_\alpha$ is a connected open subset of $\R^n$. These are known as charts
    \end{enumerate}
    $M$ is made into an $n$ dimensional \textbf{manifold} if the charts are compatible: for all $\phi_\alpha$ and $\phi_\beta$, we must have $\phi_\alpha(V_\alpha \cap V_\beta)$ open in $\R^n$ and $$\phi_\alpha \circ  \phi_\beta ^{-1}: \phi_\beta(V_\alpha \cap V_\beta) \to \phi_\alpha(V_\alpha \cap V_\beta)$$ must be smooth maps on $\R^n$.
\end{defi}

\begin{defi}[Smooth map between manifolds]
    Suppose that $M$ with charts $\phi_\alpha$ and $N$ with charts $\psi_\beta$ are smooth $m$ and $n$ dimensional manifolds respectively. A function $f: M \to N$ is smooth iff for all $\phi_\alpha$ and $\psi_\beta$, we have that $\psi_\beta \circ f \circ \phi_\alpha^{-1}$ is a smooth map from $\R^m$ to $\R^n$ in the usual sense.
\end{defi}

\begin{defi}[Diffeomorphism]
    A smooth map $f$ between two manifolds $M$ and $N$ is a \textbf{diffeomorphism} if $f^{-1}$ exists and is also a smooth map.
\end{defi}

A Lie group is a group $G$ that is also a smooth manifold. 

\begin{defi}[Lie group]\end{defi}
\begin{enumerate}
    \item A group $G$ endowed with a Hausdorff topology $\tau$ and smooth structure $(V_\alpha, \phi_\alpha)$ is a \textbf{Lie group} iff
    \begin{enumerate}
        \item The left and right multiplication maps $L_g : G \to G : h \mapsto gh$ and $R_g : G \to G : h \mapsto hg$ are smooth for every $g$.
        \item The inverse map $\inv: G \to G : g \mapsto g^{-1}$  is smooth.
    \end{enumerate}
    \item A \textbf{morphism} between Lie groups is a smooth group homomorphism.
\end{enumerate}

Noting that $L_g$ and $R_g$ have smooth inverses $L_{g^{-1}}$ and $R_{g^{-1}}$, we see that these maps are in fact diffeomorphisms.

The first example of a Lie group is the general linear group $GL(V)$, where the manifold structure inherited from $\R^{n^2}$. This allows us to define representations of a Lie group.

\begin{defi}[Lie Group Representation]
    For a given Lie group $G$, a representation on a vector space $V$ is a Lie group morphism $\pi: G \to GL(V)$.
\end{defi}

There are several other important examples of Lie groups. In each of the following, the manifold structure is inherited from $\C^{n^2}$ or $\R^{n^2}$. 

\begin{enumerate}
    \item $U(n)$ is the group of $n\times n$ complex unitary matrices 
    $$U(n) = \{X \in GL(n, \C) : XX^\dagger = 1\}.$$
    \item $O(n)$ is the group of $n\times n$ real orthogonal matrices 
    $$U(n) = \{X \in GL(n, \R) : XX^T = 1\}.$$
    \item $SL(n, \C)$ is the group of $n\times n$ complex matrices with determinant 1:
    $$SL(n, \C) = \{X \in GL(n, \C) : \det(X) = 1\}.$$
    \item $SL(n, \R)$ is the group of $n\times n$ real matrices with determinant 1:
    $$SL(n, \C) = \{X \in GL(n, \R) : \det(X) = 1\}.$$
    \item $SU(n)$ is the intersection of $SL(n, \C)$ and $U(n)$:
    $$SU(n) = \{X \in GL(n, \C) : XX^\dagger = 1, \det(X) = 1\}.$$
    \item $SO(n)$ is the intersection of $SL(n, \R)$ and $O(n)$:
    $$SU(n) = \{X \in GL(n, \R) : XX^T = 1, \det(X) = 1\}.$$
\end{enumerate}

Although Lie groups can often be very large, the following theorem shows that they can be characterised by an arbitrarily small open set.

\begin{thm}
    Suppose $G$ is a connected Lie group, and $U \subseteq G$ is any neighborhood of the identity. Then $U$ generates $G$.
\end{thm}

\begin{proof}
First of all, note that any neighborhood of a point contains an open neighborhood of that point, so if necessary we replace $U$ with open subset. $U^{-1}$ is also an open neighborhood of the identity, so if necessary, we further replace $U$ with $U \cap U^{-1}$ such that $U=U^{-1}$.

Let $S$ be the set generated by $U$. For any $g \in S$, the left action of $g$ is a diffeomorphism, so $gU$ is open because $U$ is. Now write $H = G \setminus S$. If $hu \in S$ for any $u \in U$, $h \in H$, then $huu^{-1} = h$ is also in $S$, so we must have that $hU$ is disjoint with $S$. Hence as $hU$ is also open, $H' = \bigcup_{h \in H} hU$ is a open subset of $G$ that is disjoint from $S$ with $G = H' \cup S$. By the connectedness of $G$, since $S$ is nonempty, we must have $G=S$. 
\end{proof}

It turns out that we can extend this idea further. The region around the identity of the Lie group can in fact be `Linearised' to form a Lie algebra that completely characterizes the connected component of the identity. This is the focus of the following two sections.

\subsection{Lie Algebras}
We begin with the definition of a Lie algebra.

\begin{defi}[Lie algebra]\end{defi}

\begin{enumerate}
    \item A vector space $V$ over a field $\F$ is a \textbf{Lie algebra} if it is endowed with a multiplication $V \times V \to V$ denoted $(x, y) \mapsto \bracket{x, y}$ such that
    \begin{enumerate}
        \item The bracket operation is bilinear.
        \item $\bracket{x, x} = 0$ for all $x \in V$. 
        \item $\bracket{x, \bracket{y, z}} + \bracket{y, \bracket{z, x}} + \bracket{z, \bracket{x, y}} = 0$
    \end{enumerate}
    The third requirement is known as the Jacobi identity.
    \item A \textbf{morphism} between Lie algebras $\g$ and $\h$ is a linear function $\phi : \g \to \h$ such that 
    $$\phi(\bracket{X, Y}) = \bracket{\phi(X), \phi(Y)}.$$
\end{enumerate}

The first thing to note is that applying requirements $(a)$ and $(b)$ to the bracket $\bracket{x + y, x + y}$, we see that $\bracket{x, y} = -\bracket{y, x}$, so the bracket is anti commutative. We also note that in general the bracket does not have to be associative (the Jacobi identity can be used to show that associativity will hold only when $\bracket{a, \bracket{b,c}} = 0)$ for all $a, b, c$).

Our first example of a Lie algebra is a very important one.

\begin{defi}[General linear algebra]
    For a given vector space $V$, the set of all endomorphisms on $V$ form an algebra with scalar multiplication from the field $\F$ in $V$. This vector space can be made into the \textbf{general linear algebra} $\gl(V)$ by replacing the standard product with a bracket operation where 
    $$\bracket{x, y} = xy - yx$$
    for all $x, y \in V$.
\end{defi}

Representations are then defined in the usual way. 

\begin{defi}[Representation of a Lie algebra]
    Suppose $\g$ is a Lie algebra. A \textbf{representation} of $\g$ on a vector space $V$ is a Lie algebra morphism 
    $\pi : \g \to \gl(V).$
\end{defi}

\subsection{The Lie Algebra of a Lie group}
This section illustrates how a Lie algebra arises from the tangent space at the identity of a Lie group.

For any $g \in G$, we define the conjugation action $\Psi_g: G \to G$ by $\Psi_g(h) = L_g \circ R_g (h) = ghg^{-1}$.  Since $\Psi_g$ fixes the identity $e$ in $G$, the derivative $(d\Psi_g)_e : T_e G \to T_e G$ is an endomorphism on $T_e G$. As the composition of diffeomorphisms, $\Psi_g$ is a diffeomorphism. Therefore, $d\Psi_g^{-1}$ is non singular, so $(d\Psi_g)^{-1}_e$ exists. Therefore $(d\Psi_g)_e$ is actually an automorphism on $T_e G$. This motivates the following definition.

\begin{defi}[Adjoint representation of a Lie group]
    Suppose $G$ is a Lie group. The \textbf{adjoint representation} $\Ad: G \to \Aut(T_e G)$ of $G$ is the map $\Ad(g) = (d\Psi_g)_e$.
\end{defi}

Now $\Aut(T_e G)$ is an open subset of $\End(T_e G)$, so the tangent space of $\Aut(T_e G)$ at the identity $I = \id_{T_e G}$ can be naturally identified with $\End(T_e G)$. As such, the derivative of $\Ad$ is a map
$$\ad = (d\Ad)_{I}: T_e G \to \End(T_e G).$$

\begin{thm}
    For a Lie group $G$, the vector space $T_e G$ is made into a Lie algebra $\g$ by the bilinear map
    $$\bracket{\cdot, \cdot} : \g \times \g \to \g : \bracket{x, y} = \ad(x)(y).$$
\end{thm}

Instead of providing a complete proof, we show that this definition corresponds to the commutator when $G = GL(\R^n)$.

\begin{proof}
The first thing to note is that when $G = GL(\R^n)$, the exponential map $\exp: T_e G \to G: X \mapsto \sum_{n=0}^{\infty} X^n/n!$ is well defined. Therefore, for any $g \in G$, and $X \in T_e G$,
\begin{align*}
\Ad(g)(X) &= \left. d\Psi_g \right|_e (X)\\
    &= \left. \frac{d}{dt} \Psi_g(\exp(tX)) \right|_{t=0} \\
    &= \left. \frac{d}{dt} g\exp(tX)g^{-1} \right|_{t=0} \\
    &= \left. gA \exp(t X)g^{-1} \right|_{t=0}\\
    &= gXg^{-1}.
\end{align*}
We then see that for any $X, Y \in T_e G$,
\begin{align*}
\ad(X)(Y) &= \left. d\Ad \right|_e (X)(Y)\\
    &= \left. \frac{d}{dt} \Ad(\exp(tX))(Y)\right|_{t=0} \\
    &= \left. \frac{d}{dt} \exp(tX)Y\exp(tX)^{-1}\right|_{t=0} \\
    &= \left. \frac{d}{dt} \exp(tX)Y\exp(-tX)\right|_{t=0} \\
    &= \left. X\exp(tX)Y\exp(-tX) - \exp(tX)Y X \exp(-tX)\right|_{t=0} \\
    &= XY - Y X.
\end{align*}
\end{proof}

It turns out that we can define an exponential map even in more general situations. Suppose $G$ is a Lie group with Lie algebra $\g$. For any $X \in \g$, we can define a vector field $v_{X}$ by $V_X(g) = \left. dL_g \right|_e (X)$, where $L_g$ is the left action of $g$. Integrating $v_X$, we get a curve $\phi$ from an open neighborhood of 0 $I$ to $G$. According to \cite{fulton_harris}, $\phi$ is a homomorphism of Lie groups, so it extends to the entire real line. We can now define the exponential map.
\begin{defi}[Exponential map]
    Suppose $G$ is a Lie group with Lie algebra $\g$. The exponential map $\exp : \g \to G$ is defined by 
    $$\exp(X) = \phi_X(1),$$
    where $\phi_X$ is the integral curve described above. 
\end{defi}

According to \cite{fulton_harris}, there is also a one to one correspondence between morphisms of two Lie groups and morphisms of their associated Lie algebras.

\begin{prop}
    Suppose $G$ and $H$ are Lie groups with $G$ simply connected. A morphism $\pi: G \to H$ is uniquely determined by its differential $\pi' := \left. d\pi \right|_e : \g \to \h$. Furthermore, these maps commute with the exponential: 
    $$\pi(\exp(X)) = \exp(\pi'(X))$$
    for any $X \in \g$.
\end{prop}

This is particularly useful because it implies that a Lie group representation $\pi$ induces an associated Lie algebra representation $\pi'$. We will use this many times in the section on Quantum mechanics.

\subsection{Lie algebras of U(n), SL(n, \texorpdfstring{$\C$}{C}) and SU(n)}

If we write $\mathfrak{u}(n)$ for the Lie algebra of $U(n)$, then for all $X \in \mathfrak{u}(n)$ and $t \in \R$, $e^{tX} \in U(n)$, so $e^{tX}e^{tX^\dagger} = 1$. Differentiating this expression with respect to $t$ yields the condition $X + X^\dagger = 0$. Note that this argument also holds for infinite dimensional unitary groups. In any case, we see that elements of $\mathfrak{u}(n)$ must be skew adjoint matrices. This inspires the following proposition. 

\begin{prop}
    The Lie algebra $\mathfrak{u}(n)$ of $U(n)$ is precisely the set of skew adjoint $n \times n$ complex matrices.
\end{prop}

\begin{proof}
    For a skew adjoint complex matrix, diagonal entries are imaginary and entries in the upper triangular section are completely determined by the lower triangular section. Therefore, there are $n + 2 \frac{n^2 - n }{2} = n^2$ real parameters in such a matrix.

    On the other hand, because the tangent space has the same dimension as the manifold, the dimension of $\mathfrak{u}(n)$ must be the same as $U(n)$. Unitary matrices are embedded in $2n^2$ real dimensional space, but are subject to $n$ real constraints normalizing the columns, and $\frac{n^2 - n }{2}$ complex constraints requiring that columns are orthogonal. Therefore $\mathfrak{u}(n)$ has dimension $2n^2 - n - 2 \frac{n^2 - n }{2} = n^2$ as well. 
    
    Therefore, every skew adjoint matrix must also be in $\mathfrak{u}(n)$ because $\mathfrak{u}(n)$ is a subset of the skew adjoint matrices with the same dimension.
\end{proof}

In the case of $SL(n, \C)$, we can again use the exponential map to examine the Lie algebra: for any element $X \in \sl(n, \C)$, we have $\det(e^X) = 1$, so as $\det(e^X) = e^{\tr(X)}$, we have that $\tr(X) = 0$. 

\begin{prop}
    The Lie algebra $\sl(n, \C)$ of $SL(n, \C)$ is precisely the set of traceless $n \times n$ complex matrices. 
\end{prop}

\begin{proof}
    As both the trace and determinant map a matrix to the complex numbers, both conditions impose two constraints on the matrices. 
    
    Therefore, every traceless $n \times n$ complex matrix must also be in $\sl(n, \C)$ because $\sl(n, \C)$ is a subset with the same dimension.
\end{proof}

As $SU(n)$ is the intersection of $SL(n, \C)$ and $U(n)$, the lie algebra $\su(n)$ of $SU(n)$ is just the intersection of $\sl(n, \C)$ and $\mathfrak{u}(n)$. 

\begin{prop}
    $SL(n, \C)$ is the set of $n \times n$ complex skew adjoint traceless matrices: $$\su(n) = \{X \in M(n, \C) : X^\dagger = -X, \tr(X) = 0\}.$$
\end{prop}

In the case where $n = 2$, there is a nice relationship between $\su(n)$ and $\sl(n, \C)$. First of all, we can write

$$\su(2) = \left\{\begin{pmatrix}-ix & y+iz \\ -y + iz & ix\end{pmatrix} : x, y, z \in \R\right\}.$$

On the other hand, since $\sl(2, \C)$ is just the set of traceless $2 \times 2$ complex matrices, so an element of $\sl(2, \C)$ is of the form
$$
\begin{pmatrix}x-i y & a+i b \\ c + id & -x+i y\end{pmatrix} = \begin{pmatrix}-i y & s+i u \\ -s + i u & i y\end{pmatrix} + i\begin{pmatrix}-i x & v+i t \\ -v + i t & i x\end{pmatrix}
$$
where $s = \frac{1}{2}(a - c)$, $t = -\frac{1}{2}(a + c)$, $u = \frac{1}{2}(b + d)$ and $v = \frac{1}{2}(b - d)$. Hence, it is not hard to see that when viewed as a vector space over $\R$, $\sl(2, \C) \simeq \su(2) \oplus i \su(2)$. 

To make this situation a little clearer, we give a basis for $\su(2)$ that relates nicely to a basis for $\sl(2, \C)$. It is not too hard to see that $\su(2) = \text{span}_{\R}\{X_1, X_2, X_3\}$, where
$$X_1 = \begin{pmatrix}0 & i \\ i & 0\end{pmatrix}, \hspace{2em} X_2 = \begin{pmatrix}0 & 1 \\ -1 & 0\end{pmatrix}, \hspace{2em} X_3 = \begin{pmatrix}-i & 0 \\ 0 & i\end{pmatrix}.$$
These matrices obey the commutation relations
$$ \bracket{X_1, X_2} = X_3, \hspace{2em} \bracket{X_2, X_3} = X_1, \hspace{2em} \bracket{X_3, X_1} = X_2.$$
We can then define ${e} = \frac{1}{2}(X_2 - iX_1)$, ${f} = -\frac{1}{2}(X_2 + iX_1)$ and ${h} = iX_3$ such that
$${e} = \begin{pmatrix}0 & 1 \\ 0 & 0\end{pmatrix}, \hspace{2em} {f} = \begin{pmatrix}0 & 0 \\ 1 & 0\end{pmatrix}, \hspace{2em} {h} = \begin{pmatrix}1 & 0 \\ 0 & -1\end{pmatrix}.$$
These matrices obey the commutation relations
$$\bracket{{h}, {f}} = -2{f}, \hspace{2em} \bracket{{h}, {e}} = 2{e}, \hspace{2em} \bracket{{f}, {e}} = -{h}.$$
Furthermore, $\sl(2, \C) = \text{span}_{\R}\{X_1, X_2, X_3, iX_1, iX_2, iX_3\} = \text{span}_{\C}\{{e}, {f}, {h}\}$.

\subsection{Complex Irreps of \texorpdfstring{$\sl(2, \C)$}{sl(2, \C)} and \texorpdfstring{$\su(2)$}{su(2))}}

Suppose that $\sl(2, \C)$ acts linearly on a complex finite dimensional vector space $V$. We define the \textbf{weight space} $V_\lambda = \{v \in V : {h}v = \lambda v\}$ for a given \textbf{weight} $\lambda$. Clearly, if $\lambda$ is not an eigenvalue of ${h}$, then $V_\lambda = \{0\}$.

\begin{prop}
    For any $v \in V_\lambda$, ${f} v \in V_{\lambda - 2}$ and ${e} v \in V_{\lambda + 2}$.
\end{prop}

\begin{proof}
    According to the commutation relations given in the above section, ${h}{f} - {f}{h} = -2{f}$, so ${h}({f}v) = ({f}{h} - 2{f}) v = (\lambda - 2){f}v$. Similarly, ${h}{e} - {e}{h} = 2{e}$, so ${h}({e}v) = ({e}{h} + 2{e}) v = (\lambda - 2){e}v$.
\end{proof}

As $\C$ is algebraically closed, ${h}$ must have at least one eigenvalue with an eigenvector $u$. Therefore, by finite dimensionality, there must be an eigenvector $v$ of ${h}$ such that ${e} v = 0$. We say that such an eigenvector is a \textbf{maximal vector} of $V$ and that the corresponding eigenvalue $\lambda$ is a \textbf{maximal weight}. Similarly, we say that an eigenvector $w$ of ${h}$ such that ${f} w = 0$ is a \textbf{minimal vector}, with \textbf{minimal weight}.

\begin{thm}[Irreducible representations of $\sl(2, \C)$]
    Let $V$ be an $n$ dimensional complex vector space with an irreducible representation $\pi$ of $\sl(2, \C)$. Then $V$ decomposes into a sum of one dimensional weight spaces 
    $$V_{-l}, V_{-l + 2}, \hdots, V_{l -2}, V_{l},$$
    where $l = n - 1$ is an integer.
\end{thm}

\begin{proof}
    Repeatedly applying $\pi(f)$ to the highest weight vector $v_{\lambda}$, we get a set $\beta$ of $k+1$ eigenvectors of $\pi(h)$:
    $$\beta = \{v_{\lambda}, v_{\lambda -2}, v_{\lambda -4}, \hdots, v_{\lambda - 2k} = w\}$$ 
    where $v_{\lambda -2j} = \pi(f)^j v_{\lambda}$ and $w$ is the minimal vector. Let $W = \text{span}_{\C}(\beta) \subseteq V$. Then we claim that $W$ is invariant.

    As $\pi(f)(\beta) = \{v_{\lambda -2}, v_{\lambda -4}, \hdots, w\} \subset \beta$, $W$ is invariant under the action of $\pi(f)$. Similarly, $\beta$ is a set of eigenvectors of $\pi(h)$, so $W$ must be $\pi(h)$ invariant. We now use induction to show that for all $j \leq k + 1$, (where $v_{\lambda - 2(k+1)} = 0$)
    $$\pi(e)v_{\lambda - 2j} = j(\lambda-j+1)v_{\lambda - 2(j-1)}.$$ 
    Since $v_{\lambda}$ is the highest weight vector, $\pi(e)v_{\lambda} = 0$, so the statement holds for $j=0$. Assuming validity for $j \leq k$, we see that 
    \begin{align*}
        \pi(e) v_{\lambda-2(j+1)} &= \pi(e) \pi(f) v_{\lambda-2 j} \\
            &=(\pi([e, f])+\pi(f) \pi(e)) v_{\lambda-2 j} \\
            &=(\pi(2 h)+\pi(f) \pi(e)) v_{\lambda-2 j} \\
            &=((\lambda-2 j) v_{\lambda-2 j}+\pi(f) j(\lambda-j+1) v_{\lambda-2(j-1)}\\
            &=((\lambda-2 j)+j(\lambda-j+1)) v_{\lambda-2 j} \\
            &=(j+1)(\lambda-(j+1)+1) v_{\lambda-2((j+1)-1)}.
    \end{align*}
    Hence $W$ must also be $\pi(e)$ invariant, and therefore also invariant under the action of any $x \in \sl(2, \C)$. However, because $V$ is irreducible, this implies that $W=V$ and $k=l$.

    Finally, note that $\pi(e)v_{\lambda-2(l+1)} = \pi(e)\pi(f)w = 0$. Therefore, $$(l + 1)(\lambda-(l+1)+1) = (l + 1)(\lambda-l) = 0,$$
    so $\lambda = l$, and the theorem is proved.
\end{proof}

This classification carries over to $\su(2)$ in the following way. If $\pi$ is a finite dimensional complex irreducible representation of $\su(2)$ on $V$, then $\pi^*$ defined by $\pi^*({h}) = i\pi(X_3), \pi^*({e}) = \frac{1}{2}\pi(X_2) - \frac{i}{2}\pi(X_1)$, and $\pi^*({f}) = -\frac{1}{2}\pi(X_2) - \frac{i}{2}\pi(X_1)$ is an irrep of $\sl(2, \C)$. Therefore, $V$ decomposes in the way described above, and each of the weight spaces $V_m$ are also eigenspaces of $\pi(X_3)$.

\subsection{Complex Irreps of SO(3) and SU(2)}

Now that we have classified the complex irreps of $\su(2)$, we can use the exponential map to find the complex irreps of $SU(2)$. Explicitly, since $SU(2)$ is simply connected, any representation $\pi$ on $V$ is completely determined by its differential $\pi'$, which is a representation of $\su(2)$. Therefore, $V$ must decompose in the way described in the above section. 

We now want to find the irreps of $SO(3)$. A basis for $\so(3)$ is given by 
$$l_1 = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \\ \end{pmatrix}, \hspace{1em} l_2 = \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0 \\ \end{pmatrix}, \hspace{1em} l_3 = \begin{pmatrix} 0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \\ \end{pmatrix}.$$
These matrices obey the commutation relations
$$ \bracket{l_1, l_2} = l_3, \hspace{2em} \bracket{l_2, l_3} = l_1, \hspace{2em} \bracket{l_3, l_1} = l_2.$$
Noting that this is the same set of relations as for $\su(2)$, we can define a Lie algebra isomorphism $\pi' : \so(3) \to \su(2) : X_j \mapsto l_j$. This means that the irreps of $\so(3)$ are the same as those for $\su(2)$. Importantly however, this does not mean that the irreps of $SO(3)$ are the same as those for $SU(2)$. Using the exponential map, we can define a Lie group homomorphism $\pi : SO(3) \to SU(2)$ by
$$\pi\right(\exp\left(\vec{n} \cdot \vec{l}\right)\right) = \exp\left(\vec{n} \cdot \pi'\left(\vec{l}\right)\right),$$


It turns out that there is a nice relationship between $SO(3)$, $SU(2)$ and the unit quaternions. Any element $g \in SU(2)$ has the form
$$g = \begin{pmatrix}z & w \\ -\bar{w} & \bar{z}\end{pmatrix}$$
where $z, w \in \C$ and $|z|^2 + |w|^2 = 1$. Writing $z = a + i b, w = c + i d$, we see that $a^2 + b^2 + c^2 + d^2 = 1$. This equation defines the three sphere $S^3$ in $\R^4$, which is topologically equivalent to the unit sphere in the quaternions \cite{wiki}. In fact, we can define $\phi: SU(2) \to \mathbb{H}$, by 
$$\phi\begin{pmatrix}a + i b & c + i d \\ -c + i d & a - i b\end{pmatrix} = a + bi + cj + dk.$$
Routine calculations show that this is an isomorphism. 

For any point $(x, y, z) \in \R^3$, we can associate an imaginary quaternion $q = xi + yj + zk$. Suppose that $g \in SU(2)$ has $\phi(g)$ as its associated quaternion. Then more calculations show that $\phi(g)q\phi(g)^{-1} = u i + v j + w k$ is another imaginary quaternion, which can be associated with a point $(u, v, w)$ in $\R^3$. According to \cite{wiki}, this gives us a $2: 1$ surjective homomorphism $\Phi : SU(2) \to SO(3)$. In particular, note that $\Phi(-g)(q) = \phi(-g)q\phi(-g)^{-1} = \phi(g)q\phi(g)^{-1} = \Phi(g)(q)$, so $\Phi(-g) = \Phi(g)$. In this situation we say that $SU(2)$ is a double cover of $SO(3)$.

The presence of $\Phi$ allows us to deduce the irreducible representations of $SO(3)$. 

The net result of this is that there is exactly one complex irreducible representation of $SO(3)$ for each odd dimension.

\section{Quantum Mechanics}
\subsection{The Quantum Formalism}
Classically, a physical system consisted of the following components: 
\begin{enumerate}
    \item A set $S$ of possible states. This set was a finite dimensional real vector space, with each coordinate corresponding to the value of a particular quantity. The original example of this is description of a single particle in Newtonian mechanics, in which $S$ is a two dimensional real vector space, with the first dimension corresponding position and the second to velocity.
    \item A set $\Omega$ of observable quantities. These quantities were expressed as a function from $S$ to $\R$. For example, if a particle of mass $m$ was modeled in a Newtonian system, then the the energy of a particle would be given as $E: S \to \R : E(x, v) = mv^2/2$.
    \item A rule which describes how the state evolves through time. For example, if a particle with mass $m$ under the influence of a constant force $F$ was modeled by Newtonian mechanics, then $(\dot{x}, \dot{v}) = (v, \frac{F}{m})$.
\end{enumerate}
Embedded in component $2$ is the assumption that every observable must always be completely described by a single real value. Unfortunately however, several experiments in the early 20th century showed this to be insufficient when modeling systems which are precise enough to deal with individual particles. As a result, the founders of quantum mechanics developed a system where
\begin{enumerate}
    \item $S$ is replaced with a Hilbert space $\H$. Examples include taking $\H = L^2(\R^3)$ to model a spin-less particle in three dimensions, $\H = \C^2$ for a two state system like the spin of an electron, $\H = L^2(\R) \otimes L^2(\R)$ for two particles in one dimension, and $\H = L^2(\R) \otimes \C^3$ for a particle in one dimension with three possible spin states.
    \item Observable quantities become hermitian operators on $\H$. To see what this means in a physical context, note that hermitian operators are diagonalizable. Therefore, specifying a hermitian operator is equivalent to specifying a set of real eigenvalues (the possible `well defined' values of the observable) and an orthonormal basis for $\H$ (the states corresponding to those eigenvalues). The benefit of this  description is that it allows for the state of a particle to have nonzero components in multiple eigenspaces - i.e. the particle can be in a `superposition' of eigenstates. 
    \item The equation defining evolution of the state through time is replaced with the Schr\"{o}dinger equation: 
    $$i\hbar\frac{d}{dt}\ket{\psi(t)} = H\ket{\psi(t)},$$
    where $H$ is a distinguished observable called the Hamiltonian, and $\hbar$ is plank's constant divided by $2 \pi$.
\end{enumerate}

An important consequence of $2$ is that if a system is in state $\ket{\phi} \in \H$, then all measurable quantities will be unchanged if the state changes to $\lambda\ket{\phi}$ for any scalar $\lambda \neq 0$ (Note that $\ket{0}$ does not correspond to a physical state, because it has no components in any eigenspace of any observable). As such, a state is actually better associated with a vector in $\H / \C$.

When the eigenvalue spectrum of an operator is discrete, we find that the corresponding measurable becomes `quantized', and cannot take on the full range of values we would expect from classical mechanics. This is however, not always the case - it is common for observables such as position $X$ and momentum $P$ to have a continuous spectrum of eigenvalues. Unfortunately there is considerably more technical detail required for a precise mathematical treatment of this situation. For example, a continuous spectrum of eigenvalues would require that there is also an continuous set of orthonormal eigenvectors, which is not possible in typical Hilbert spaces such as $\C^n$ or $L^2(\R^3)$. The physics approach usually handles this case by pretending that the Hilbert space also contains an object known as the Dirac delta function $\delta$, which is zero everywhere except for at point where it is infinite. A more mathematical approach requires the notion of a rigged Hilbert space, which we do not discuss.

Before proceeding further, we consider an easy example. Suppose that $\H$ is two dimensional and $H$ has eigenvectors $\ket{\psi_1}$ and $\ket{\psi_2}$, with eigenvalues $E_1$ and $E_2$ respectively. Our aim is to find $\ket{\psi(t)}$ for any $t$. We start by writing $\ket{\psi(t)} = \alpha(t) \ket{\psi_1} + \beta(t) \ket{\psi_2}$, for arbitrary functions $\alpha, \beta$. Applying the Schr\"{o}dinger equation, 
$$i\hbar \alpha'(t) \ket{\psi_1} + i\hbar \beta'(t) \ket{\psi_2} = E_1 \alpha(t) \ket{\psi_1} + E_2 \beta(t) \ket{\psi_2}.$$
Equating components, we see that $\alpha(t) = Ae^{-iE_1t/\hbar}$ and $\beta(t) = Be^{-iE_2t/\hbar}$ for constants $A, B$ determined by the initial conditions. The solution is therefore $\ket{\psi(t)} = Ae^{-iE_1t/\hbar}\ket{\psi_1} + Be^{-iE_2t/\hbar}\ket{\psi_2}$. 

This example highlights an important consequence of the linearity of the Schr\"{o}dinger equation: eigenstates of $H$ evolve independently through time. It turns out that this simple observation is the source of significant philosophical debate, which we discuss briefly in the next section.

\subsection{Measurement}
The formalism of quantum mechanics becomes useful when it can be applied to deduce results of experiments that can be conducted by humans. The problem is that any measurement system (which necessarily includes the people doing the measuring) has at least $\sim 10^{27}$ atoms. Unfortunately, it is impractical to model the evolution of such a system with Schr\"{o}dinger's equation. Physicists wanting to make useful predictions therefore imagine that when a quantum system interacts with a `measuring device', the system collapses into a well defined state. This is known as the Born rule.

\begin{princ}[Born Rule]
    Suppose a particle is in the state $\ket{\psi} = \sum \alpha_\omega \ket{\omega}$ where $\ket{\omega}$ are the eigenvectors of an observable $\Omega$. Measurement of $\Omega$ will yield the value $\omega$ with probability $$P(\omega) = \frac{\abs{\anglet{\omega}{\psi}}^2}{\anglet{\psi}{\psi}} = \frac{|\alpha_\omega|^2}{\sum |\alpha_\omega|^2}.$$ This measurement will cause the state of the particle to change from $\ket{\psi}$ to $\ket{\omega}$.
\end{princ}

Another perspective says that when a measurement is performed, the quantum system must simply be expanded to include the state of the measuring device. For example, if an experiment to determine the outcome of a `quantum' coin flip was conducted, the resulting state would be $$\alpha \ket{H}\otimes\ket{\text{H seen}} + \beta \ket{H}\otimes\ket{\text{T seen}} + \gamma \ket{T}\otimes\ket{\text{H seen}} + \delta \ket{T}\otimes\ket{\text{T seen}}$$ for some constants $\alpha, \beta, \gamma, \delta$ ($\beta$ and $\gamma$ are probably very small). In reality, the dimensionality of the new system is actually very high. As a result, each of these four `macroscopic' states will be very close to an eigenstate of the hamiltonian $H$. As eignstates of $H$ evolve independently from each other, we therefore see that each of these four states are effectively isolated from each other. This process is studied more rigorously in the subject of decoherence, which has been an active area of research since the 1980's.

Under this perspective, there was no `collapse' - all of the outcomes of the experiment did in fact occur, but they are no longer interacting. This has a number of troubling consequences, most notably that there are an unimaginably number of slightly different `versions' of reality. 

It can therefore be argued that the notion of collapse (the Copenhagen interpretation) is `simpler' because it doesn't require the existence of infinite noninteracting states which we can never detect. On the other hand it can also be argued that `no collapse' (the many worlds interpretation) is simpler because it doesn't require a poorly defined, non-deterministic process to be a part of the theory. Clearly, this becomes a matter of philosophy.

\subsection{Lie Theory, Representations in Quantum Mechanics}
It turns out that for a quantum system modeled by a Hilbert space $\H$, most of the interesting observables arise from the symmetries of $\H$. In this section, we give a outline of how this works, and the remaining sections examine several examples.

Suppose that a compact group $G$ has a strongly continuous unitary representation $\pi: G \to U(\H)$ for some Hilbert space $\H$. We say that $G$ is a group of symmetries for $\H$ because it preserves the inner product (and therefore also the probabilities of various physical measurements). According to the Peter Weyl theorem, $\H$ decomposes into a direct sum of finite dimensional unitary representations of $G$.  

In the section on Lie theory, we showed that the lie algebra $\mathfrak{u}(n)$ of $U(n)$ is the set of $n \times n$ complex skew adjoint matrices. Furthermore, we showed that any Lie group representation $\pi$ has an associated Lie algebra representation $\pi'$. Therefore, for any $X \in \g$, $\pi'(X)$ is a skew adjoint, and acts on $\H$, so $i\pi'(X)$ is a hermitian operator (observable) on $\H$. 

These observable transform in a particularly nice way under the action of $\pi(G)$ on $\H$. $\pi(g)\pi'(X)\pi(g)^{-1} = \pi'(gXg^{-1})$.

To summarize, the action of a Lie group on a Hilbert space gives rise to a set of observables that respect the symmetries encapsulated by that Lie group. As we will see in the following sections, it just so happens that the observables generated in this manner turn out to be the observables of most interest to us.

\subsection{Orbital Angular Momentum}
Suppose that we have a Hilbert space $\H$ that is modeling a system in three dimensional space. Then the natural unitary representation of $SO(3)$ on $\H$ will generate the angular momentum operators. 

To illustrate this process, we are going to study a single spin-less particle, which we will model by choosing $\H$ to be $L^2(\R^3, \C)$. In this situation, we imagine a state $\phi \in \H$ represents a probability cloud where $|\phi(\vec{x})|^2$ is the probability of being found around the point $\vec{x}$. Then it isn't too hard to see that the representation $\pi : SO(3) \to GL(H)$ defined by
$$(\pi(R)\phi)(\vec{x}) = \phi(R^{-1}\vec{x})$$
is `natural' in the sense that it corresponds to rotating the probability cloud $\phi$ by $R$. To see that this is also a unitary representation, note that for any $R \in SO(3)$, a change of variables $\vec{x} \mapsto R\vec{x}$ has a Jacobian determinant of 1 so 
$$\anglet{\pi(R)\psi}{\pi(R)\phi} = \int \psi(R^{-1}\vec{x})^* \phi(R^{-1}\vec{x})d\vec{x} = \int \psi(\vec{v})^* \phi(\vec{v})d\vec{v} = \anglet{\psi}{\phi}.$$

As $SO(3)$ is a Lie group, we get an induced Lie algebra representation $\pi' : \so(3) \to \mathfrak{u}(\H) : l \to \left. \frac{d}{dt}\pi(e^{tl})\right|_{t=0}$. Recalling that $\mathfrak{u}(\H)$ consists of skew adjoint matrices, we see that $L = i\hbar\pi'(l)$ is a hermitian operator on $\H$ for any $x \in \so(3)$ (The factor of $\hbar$ is introduced so that the eigenvalues of $L_j$ can be expressed in SI units). 

Recall that a basis for $\so(3)$ is given by matrices $l_1, l_2$, and $l_3$, with corresponding observables $L_j = i\hbar\pi'(l_j)$.  To begin with, we find an expression for $L_j$ when using the standard basis for $\R^3$. For a given function $f \in L^2(\R^3)$,
\begin{align*}
    \pi'(l_1)f(\vec{x}) &= \left. \frac{d}{dt} \pi\left(e^{t l_1}\right)f(\vec{x}) \right|_{t=0}\\ 
    &= \left. \frac{d}{dt} f\left(\begin{pmatrix} 1 & 0 & 0 \\ 0 & \cos(t) & \sin(t) \\ 0 & -\sin(t) & \cos(t) \\ \end{pmatrix} \begin{pmatrix}x_1 \\ x_2 \\ x_3 \end{pmatrix}\right) \right|_{t=0}\\
    &= \left. \frac{d}{dt} f(x_1, x_2\cos(t) + x_3 \sin(t), -x_2\sin(t) + x_3 \cos(t)) \right|_{t=0}\\ 
    &= x_3\frac{\partial f}{\partial x_2} - x_2\frac{\partial f}{\partial x_3}.
\end{align*}
Similar calculations show that 
$$\pi'(l_2)f(\vec{x}) = \left(x_3\frac{\partial f}{\partial x_1} - x_1\frac{\partial f}{\partial x_3}\right) \text{  and  } \pi'(l_3)f(\vec{x}) = \left(x_2\frac{\partial f}{\partial x_1} - x_1\frac{\partial f}{\partial x_2}\right).$$
Therefore, expressing any $l = al_1 + bl_2 + cl_3 \in \so(3)$ as a vector $\vec{n}$ in three dimensions, we see that 
$$i\hbar\pi'(l)f(\vec{x}) = \vec{n} \cdot (\vec{x} \times i\hbar \nabla f(\vec{x})).$$
Classically, the quantity $
\vec{x} \times i\hbar \nabla f(\vec{x})$ represents the angular momentum of the wave $f$ in three dimensions. Therefore, $L = i\hbar\pi'(l)$ must correspond to the operator which measures the component of angular momentum in the direction represented by $\vec{n}$.

Now that we have an understanding of what $L$ measures, our next task is to find its eigenvectors and eigenvalues. Once this is done, the state of our particle can be expanded in the resulting eigenbasis. This will allow us to assign a probability to each of the possible values of angular momentum. 

In view of the corollary to the Peter Weyl theorem, the compactness of $SO(3)$ implies that $\H$ decomposes as a direct sum of finite dimensional vector spaces $V_n$ which are irreducible under $SO(3)$. As there is exactly one irreducible representation of $SO(3)$ for each odd dimension, we know that 
$$\H = V_1 \oplus V_3 \oplus V_5 \oplus \hdots.$$
Furthermore, we know that each $V_{2l+1}$ is spanned by a basis of $2l+1$ functions $Y^l_{-l}, Y^l_{-l + 1}, \hdots, Y^l_{l-1}, Y^l_{l}$ where $L_3 Y^l_{m} = m Y^l_{m}$, $L_{-} Y^l_{m} = Y^l_{m-1}$ and $L_{+} Y^l_{m} = Y^l_{m+1}$. 
Suppose that $Y^l_{l}$ is the highest weight vector of $V_{2l+1}$. If we transform into spherical coordinates via
\begin{align*}
    x_1&=r\sin(\theta)\cos(\phi)\\
    x_2&=r\sin(\theta)\sin(\phi)\\
    x_3&=r\cos(\theta)
\end{align*}
then the action of $SO(3)$ will leave $r$ invariant, so we can consider $Y^l_{m}$ to be a function of just $\theta$ and $\phi$. Now 
$$\left(
    \begin{array}{c}
    \frac{\partial}{\partial x_{1}} \\
    \frac{\partial}{\partial x_{2}} \\
    \frac{\partial}{\partial x_{3}}
    \end{array}\right)=\left(\begin{array}{ccc}
    \sin \theta \cos \phi & \cos \theta \cos \phi & -\sin \phi \\
    \sin \theta \sin \phi & \cos \theta \sin \phi & \cos \phi \\
    \cos \theta & -\sin \theta & 0
    \end{array}\right)\left(\begin{array}{c}
    \frac{\partial}{\partial r} \\
    \frac{1}{\partial \theta} \\
    \frac{\partial}{r \sin \theta} \frac{\partial}{\partial \phi}
    \end{array}
\right),$$
so we can write 
\begin{align*}
    L_1 &= i\hbar\left(x_3\frac{\partial}{\partial x_2} - x_2\frac{\partial}{\partial x_3}\right) = i\hbar\left(\sin(\phi)\frac{\partial}{\partial \theta} + \cot(\theta)\cos(\phi)\frac{\partial}{\partial \phi}\right)\\
    L_2 &= i\hbar\left(x_3\frac{\partial}{\partial x_1} - x_1\frac{\partial}{\partial x_3}\right) = i\hbar\left(-\cos(\phi)\frac{\partial}{\partial \theta} + \cot(\theta)\sin(\phi)\frac{\partial}{\partial \phi}\right)\\
    L_3 &= i\hbar\left(x_2\frac{\partial}{\partial x_1} - x_1\frac{\partial}{\partial x_2}\right) = -i\hbar \frac{\partial}{\partial \phi}.
\end{align*}
Further calculations show that 
$$L_{+} = \hbar e^{i\phi} \left(\frac{\partial}{\partial \theta} + i\cot(\theta)\frac{\partial}{\partial \phi}\right) \text{  and  } L_{-} = \hbar e^{-i\phi} \left(-\frac{\partial}{\partial \theta} + i\cot(\theta)\frac{\partial}{\partial \phi}\right).$$
Considering the highest weight vector $Y^l_{l}(\theta, \phi)$, we therefore have two differential equations: $$L_3 Y^l_{l}(\theta, \phi) = - -i\hbar \frac{\partial}{\partial \phi} Y^l_{l}(\theta, \phi) = l Y^l_{l}(\theta, \phi)$$ 
and 
$$L_{+} Y^l_{l}(\theta, \phi) = \hbar e^{i\phi} \left(\frac{\partial}{\partial \theta} + i\cot(\theta)\frac{\partial}{\partial \phi}\right)Y^l_{l}(\theta, \phi) = 0.$$
Solving these equations, we see that 
$$Y^l_{l}(\theta, \phi) = C_{ll} e^{il\phi}\sin^{l}(\theta)$$
for some constant $C_{ll}$. The lowering operator $L_{-}$ can then be applied repeatedly to find each $Y^l_{m} = L_{-}Y^l_{m+1}$ for $m = l, l -1, \hdots -l + 1, -l$.

To summarize, we have now found an expression in spherical coordinates of each of the eigenfunctions of $L_3$ (Note that $L_3$ is special only because of the arbitrary way in which we transformed into spherical coordinates). We could therefore express any function in $\H$ in terms of these eigenfunctions, allowing for a concrete description of the state of the particle.

In a spherically symmetric potential, the Hamiltonian operator $H$ commutes with the action of $L_3$ and $L_{\pm}$. This implies that each of the eigenspaces of $L_3$ must be fixed by $H$. However, because every eigenspace of $L_3$ is one dimensional, they must also be energy eigenspaces. As $H$ also commutes with $L_{\pm}$, we see that each $V_l$ must in fact be an energy eigenspace. As such, we can label these spaces by their energy. This is the reason that there are exactly $2n+ 1$ possible values of the orbital angular momentum at energy level $n$.

\subsection{Spin}
It is also possible to model the state of a particle with a finite dimensional Hilbert space $\C^n$. In this situation, we imagine that a vector $\phi \in \H$ represents the `intrinsic properties' of the particle. One such intrinsic property of all elementary particles is spin, which arises from the action of $SU(2)$. 

Suppose that $\H = \C^n$. We know than any representation of $SU(2)$ decomposes as a sum of irreducible representations, so we restrict ourselves to the case where $\pi : SU(2) \to GL(\H)$ is irreducible. We also want $\pi$ to represent a symmetry of $\H$, so we choose $\pi$ to be unitary. As usual, we also get a Lie algebra representation $\pi' : \su(2) \to \gl(\H)$. Fortunately, we have already classified such representations: we know that $\H$ must have a basis of $n$ eigenvectors $v_m$ such that $i\pi'(X_3)v_m = m v_m$ for $m = -n/2, -n/2 + 1, \hdots, n/2 - 1,  n/2$. 

Once agin, the operators $\pi'(X_j)$ are skew adjoint, so $i\hbar\pi'(X_j)$ is an observable. It turns out that these observables measure `spin', which is a quantum property for which there is no classical analogue (although it bears some similarities to the angular momentum of a three dimensional object spinning about an internal axis). 

In general, for any $X = a X_1 + b X_2 + c X_3 \in \su(2)$, we can write $\vec{n} = (a, b, c)$, $S = i\hbar(\pi'())$, and then 
$$O(\hat{n}) = \hat{n}\cdot \pi'(X).$$
This operator 

Now suppose that we have an observable $O$ which is a function of direction $\hat{n}$. The key insight is that measuring $O(R\hat{n})$ is the same as measuring $O(\hat{n})$ in a reference frame that has been acted on by $R$. Therefore, for any $\phi, \psi \in \H$, 
$$\anglet{\phi}{O(R\hat{n})\psi} = \anglet{\rho(R)\phi}{O(\hat{n})\rho(R)\psi}= \anglet{\phi}{\rho(R)^\dagger O(\hat{n})\rho(R)\psi}$$
Hence, any such observable must transform as  
$$O(R\hat{n}) = \rho(R)^\dagger O(\hat{n})\rho(R) = \rho(R)^{-1} O(\hat{n})\rho(R)$$
as $\rho$ is a unitary representation.

Then we will need to describe an operator $O(\hat{n})$ on the Hilbert space $\H$. From a rotated reference frame corresponds to $SO(3)$ acting on $\hat{n}$ and $\H$ simultaneously. As the physical properties of the system should be independent of the direction from which we view the system, if we measure the angular momentum along $R\hat{n}$

\subsection{Tensor products and the periodic table}
In the proceeding two sections, we examined an infinite dimensional Hilbert space $L^2(\R^3)$, which is used to model a particle in space, and a finite dimensional Hilbert space $\C^n$ which is used to model the intrinsic properties of a particle. We would now like to be able to do both at once. This is done by constructing a new Hilbert space $\H = L^2(\R^3) \otimes \C^n$, which is used to model particles in three dimensional space with multiple possible spin states.

The most important example of this is that of the electron. We know that the electron has two possible spin states, so we use $\H = L^2(\R^3) \otimes \C^2$. In this situation, the operators which we had constructed on $L^2(\R^3)$ will leave the $\C^2$ subspace invariant and vise versa. The result of this is that the Hamiltonian and angular momentum operators commute with the spin operators, allowing us to break up the entire state into one dimensional eigenspaces labeled by energy, angular momentum and spin. 

These states are the source of many basic facts about the periodic table. For example, the number of electrons per orbital is $2, 6, 10, 14, \hdots$. These numbers correspond to the fact that there are $1, 3, 5, 7, \hdots$ possible values of the angular momentum, and two possible spin states for each energy level.

\bibliography{sources} 
\bibliographystyle{ieeetr}

\end{document}